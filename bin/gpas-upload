#!/usr/bin/env python3

"""
gpas-upload script
"""

import argparse, shutil
from pathlib import Path
import json
import sys

import pandas

import gpas_uploader

parser = argparse.ArgumentParser(description="GPAS batch upload tool")
parser.add_argument("--parallel", action="store_true", default=False)
parser.add_argument("--json", action="store_true", help="whether to write text or json to STDOUT")
parser.add_argument("--tags", default=None, help='plaintext file of allowed tags with one tag per line')
parser.add_argument("--token", default=None, help='the token.tok file downloaded from the GPAS user portal')
parser.add_argument("--environment", default='dev', help='which GPAS environment to use: dev, staging or prod')
subparsers = parser.add_subparsers(dest="command")

validate_args = subparsers.add_parser(
    "validate", help="parse and validate an upload CSV"
)
validate_args.add_argument("upload_csv")

import_args = subparsers.add_parser("decontaminate", help='remove human reads from the FASTQ files specified in the upload CSV file')
import_args.add_argument("--dir", default='/tmp/')
import_args.add_argument("--output_csv", default='sample_names.csv', help='the name of the mapping CSV to store the local->GPAS (batch,run,sample) lookup table')
import_args.add_argument("--reference_genome", default=None, help='the reference genome to pass to readItAndKeep')
import_args.add_argument("upload_csv")

submit_args = subparsers.add_parser("submit", help='submit the batch to GPAS')
submit_args.add_argument("--dir", default='/tmp/')
submit_args.add_argument("--output_csv", default='sample_names.csv', help='the name of the CSV to store the local->GPAS (batch,run,sample) lookup table')
submit_args.add_argument("--reference_genome", default=None, help='the reference genome to pass to readItAndKeep')
submit_args.add_argument("upload_csv")

download_args = subparsers.add_parser("download", help='download batch files from GPAS')
download_args.add_argument("--dir", default='.')
download_args.add_argument("--file_types", nargs='+', default=['fasta'], help='which files to download from fasta bam vcf json, default is fasta')
download_args.add_argument("--rename", action="store_true", help="rename the downloaded files using the local_sample_name")
download_args.add_argument("--dry_run", action="store_true", help="run but do not download the files")
download_args.add_argument("--output_csv", help="if specified, save the modified mapping csv with this name")
download_args.add_argument("mapping_csv", default='sample_names.csv')


if __name__ == "__main__":

    args = parser.parse_args()

    pandas.options.display.max_colwidth=150

    if args.command in ["validate", "decontaminate", "submit"]:

        samplesheet = Path(args.upload_csv)

        upload_csv = gpas_uploader.UploadBatch(samplesheet,
                                        run_parallel=args.parallel,
                                        token_file=args.token,
                                        environment=args.environment,
                                        tags_file=args.tags,
                                        output_json=args.json)

        if not upload_csv.instantiated:
            if args.json:
                print(upload_csv.instantiation_json)
            else:
                print("--> Upload CSV either does not exist or is not UTF-8. Do not pass this upload CSV to the GPAS upload client.")

        else:

            upload_csv.validate()

            if not upload_csv.valid:

                if args.json:
                    print(json.dumps(upload_csv.validation_json))
                else:
                    print(upload_csv.validation_errors)
                    print("--> Please fix the above errors and try validating again. Do not pass this upload CSV to the GPAS upload client.")

            else:

                if args.command == 'validate':
                    if args.json:
                        print(json.dumps(upload_csv.validation_json))
                    else:
                        print("--> All preliminary checks pass and this upload CSV can be passed to the GPAS upload app")

                else:
                    parent = Path(args.upload_csv).resolve().parent
                    outdir = Path(args.dir).resolve()

                    # run ReadItAndKeep on all the samples
                    upload_csv.decontaminate(outdir=outdir, run_parallel=args.parallel)

                    if not upload_csv.decontamination_successful:

                        if args.json:
                            print(json.dumps(upload_csv.decontamination_json))
                        else:
                            pandas.options.display.max_colwidth=100
                            print(upload_csv.decontamination_errors)
                            print("--> Please fix the above errors and try decontaminating again. Do not pass this upload CSV to the GPAS upload client.")

                    else:

                        if args.json:
                            print(json.dumps(upload_csv.decontamination_json))
                        else:
                            print("--> All samples have been successfully decontaminated")

                        # to avoid overwriting a sample_names.csv, rename any existing
                        # file to sample_names.csv.001. If that also exists, instead rename
                        # to sample_names.csv.002, if that already exists sample_names.csv.003
                        # and so on

                        # if a sample_names.csv already exists
                        if Path(parent / args.output_csv).exists():

                            output_stem = args.output_csv.split('.csv')[0]

                            # find out how many backup files there are
                            backup_files = parent.glob(output_stem + '.???.csv')

                            if len(list(backup_files)) == 0:

                                shutil.move(parent / args.output_csv, str(parent / output_stem) + '.001.csv')

                            else:

                                backup_files = parent.glob(output_stem + '.???.csv')

                                # create a list of their numbers
                                file_numbers = [ str(i).split(output_stem + '.')[1].split('.csv')[0] for i in backup_files ]

                                # what is the highest number?
                                highest_value = int(max(file_numbers))

                                # hence what is the new number?
                                next_value = "%03i" % (highest_value+1)

                                shutil.move(parent / args.output_csv, str(parent / output_stem) + '.' + next_value + '.csv')

                        # save the local -> GPAS (batch,run,sample) information
                        upload_csv.sample_sheet.to_csv(parent / args.output_csv, index=False)

                sys.stdout.flush()

                if args.command == 'submit':

                    upload_csv.submit()

                    if args.json:
                        print(json.dumps(upload_csv.submit_json))
                        print("--> All samples have been successfully submitted to GPAS for processing")

    elif args.command == 'download':

        if args.output_csv is not None:
            assert not Path(args.output_csv).is_file(), 'specified output CSV already exists!'

        download_csv = gpas_uploader.DownloadBatch( mapping_csv=args.mapping_csv,
                                                    token_file=args.token,
                                                    output_json=args.json,
                                                    environment=args.environment )

        download_csv.get_status()

        if not args.dry_run:
            for i in args.file_types:
                download_csv.download(filetype=i, outdir=args.dir, rename=args.rename)

        if args.output_csv is not None:
            download_csv.df.to_csv(args.output_csv)
